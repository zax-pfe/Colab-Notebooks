{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"V2_variables_attention.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"cells":[{"cell_type":"code","metadata":{"id":"FpeH-aYoa2DB"},"source":["print(\"Hello\")\n","print(\"There\")\n","a = 5\n","print(\"0.....\")\n","a = a + 9 \n","\n","print(\"General Kenoby\", a)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b089seRdXuEf"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import tensorflow as tf\n","#import os\n","from collections import Counter\n","\n","print(\"Start programme MF\")\n","# #############################\n","# Definition of Env variable # \n","# #############################\n","# Definition of the current env\n","# JUPYTER or G_COLAB\n","CURRENT_ENV = \"G_COLAB\"\n","\n","# Use on Jupyter_env only \n","if CURRENT_ENV == \"JUPYTER\":\n","    DATA_DIR = \"../../dev_GenJavaBdd/GenerationTest/\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EYp9_VqacjWN"},"source":["# Lecture des fichiers\n","get the number of sbt/bdd function"]},{"cell_type":"code","metadata":{"id":"6FKj9y4obUDu"},"source":["print(\"Importe files\")\n","if CURRENT_ENV == \"JUPYTER\":\n","    list_folder = os.listdir(DATA_DIR)\n","    num_folder = len(list_folder)-2\n","elif CURRENT_ENV == \"G_COLAB\":\n","    with open(\"bdd.txt\",\"r\") as f:\n","        num_folder = len(f.readlines())\n","    with open(\"sbt.txt\",\"r\") as f:\n","        if (len(f.readlines()) != num_folder):\n","            error(\"[ERROR] number of BDD function does not match BDD number\")\n","print(\"Number of function/Foler = \", num_folder)  # This would give length of files.\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4dzz9w1icmD8"},"source":["# Creation des listes "]},{"cell_type":"code","metadata":{"id":"2EhtHlv4XuEh"},"source":["from tqdm import tqdm\n","from time import sleep"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Gq7uxm3cybc"},"source":["print(\"List Creation\")\n","bdd = []\n","sbt = []\n","\n","#Create the list and save it as file\n","if CURRENT_ENV == \"JUPYTER\":\n","    print(\"Get the bdd and sbt list via folder\")\n","    \n","    # Sleep to prevent fail tqdm print\n","    #sleep(1)\n","    #for i  in tqdm (range(num_folder), desc=\"loading...\"):\n","    for i  in range(num_folder):\n","        path_iteration = DATA_DIR+'/Dir_Fonction_'+str(i+1)\n","        bdd_path = path_iteration+\"/BDD_Fonction_\"+str(i+1)+\".txt\"\n","        stb_path = path_iteration+\"/SBT_Generation_Output/Fonction_\"+str(i+1)+\"_SBT.txt\"\n","        bdd_txt = open(bdd_path,'r').read()\n","        sbt_txt = open(stb_path,'r').read()\n","        clean_bdd = cleaning_bdd(bdd_txt)\n","        bdd.append(clean_bdd)\n","        sbt.append(sbt_txt)\n","    #sleep(1)\n","    \n","    print(\"Save the BDD & SBT list as file\")\n","    # Save the list as file\n","    #os.remove(\"bddi.txt\")\n","    f_bdd = open(\"bdd.txt\", \"w+\")\n","    f_bdd.write('\\n'.join(bdd))\n","    f_bdd.close()\n","\n","    #os.remove(\"sbt.txt\")\n","    f_sbt = open(\"sbt.txt\", \"w+\")\n","    f_sbt.write('\\n'.join(sbt))\n","    f_sbt.close()\n","    \n","#Create the list from the txt file\n","else:\n","    print(\"Get the bdd and sbt list via the bdd.txt & sbt.txt\")\n","    with open(\"bdd.txt\") as f:\n","        tmp_bdd = f.read()\n","    bdd = tmp_bdd.split(\"\\n\")\n","\n","    with open(\"sbt.txt\") as f:\n","        tmp_sbt = f.read()\n","    sbt = tmp_sbt.split(\"\\n\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XyWabY8AXuEh"},"source":["print(bdd[3])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oZv3nl2sXuEi"},"source":["# Recuperation des valeurs max de bdd et sbt"]},{"cell_type":"code","metadata":{"id":"VqfDegGvXuEi"},"source":["def return_max_len( text ):\n","    all_len = []\n","    for unique_code in text:\n","        code_split = unique_code.split(' ')\n","        all_len.append(len(code_split))\n","    return max(all_len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"knkVgBdJXuEi"},"source":["bdd_counter =Counter([word for sentence in bdd for word in sentence.split(' ')])\n","sbt_counter =Counter([word for sentence in sbt for word in sentence.split(' ')])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KC2M2FI_XuEi"},"source":["common_words_sbt = []\n","for word in sbt_counter:\n","    occurence = sbt_counter[word]\n","    if occurence > 3500:\n","        common_words_sbt.append(word)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J5T0ZV8wXuEj"},"source":["common_words_bdd = []\n","for word in bdd_counter:\n","    occurence= bdd_counter[word]\n","    if occurence > 3500:\n","        common_words_bdd.append(word)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UcrdM0AmXuEj"},"source":["'''\n","def return_common_words(counter):\n","    common_words = []\n","    for word in counter:\n","        print(' --> ',word)\n","        occurence = counter[word]\n","        if occurence > 3500:\n","            common_words.append(word)\n","    return common_words\n","    \n","    \n","common_words_bdd = return_common_words(bdd_counter)\n","common_words_sbt = return_common_words(sbt_counter)\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6YzbWrGhXuEj"},"source":["# ---------------------------------------------------------------------------------------------------------------\n","\n","# Listing des variables "]},{"cell_type":"code","metadata":{"id":"2GhN2Zl4XuEj"},"source":["def return_listing(text,common_words): \n","    arr= [[] for _ in range(num_folder)] #pour calculer la taille max de toute les variables réunies \n","    var = [] # pour creer un vocabulaire \n","    for i,sentence in  enumerate(text):\n","        text_split = sentence.split( ' ')\n","        for j,word in enumerate (text_split):\n","            if word not in common_words:\n","                arr[i].append(word)\n","                var.append(word) \n","    return arr, var"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N3QIEmLCXuEk"},"source":["def return_variables_unique(text): \n","    var = [] # tableau contenant les variables separées par /, ex : var1/var2/...\n","    text_split = text.split(' ')\n","    for i,word in enumerate (text_split):\n","        if word not in common_words_sbt:\n","            var.append(word)\n","            var.append('/')\n","    return ''.join(var)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XY72aOGiXuEk"},"source":["variables_unique = return_variables_unique(sbt[0])\n","#print(variables_unique)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yRIkUY0vXuEl"},"source":["arr_var_bdd, variables_bdd = return_listing(bdd,common_words_bdd)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y7bycTotXuEl"},"source":["arr_var_sbt,variables_sbt= return_listing(sbt,common_words_sbt)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7yoef9Q7XuEl"},"source":["Conversion des variables en sentences, puis on transforme ce tablea de sentences en dataframe pour pouvoir appliquer les sostoken et eosteoken"]},{"cell_type":"code","metadata":{"id":"pQMLPdaHXuEm"},"source":["def return_var_list(variables_arr):    \n","    varlist = []\n","\n","    for vars in variables_arr:\n","        temp_varlist = []\n","        for var in vars:\n","            temp_varlist.append(var)\n","            temp_varlist.append('/')\n","        varlist.append(''.join(temp_varlist))\n","    return varlist"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FYHC3qqwXuEm"},"source":["varlist_bdd = return_var_list(arr_var_bdd)\n","varlist_sbt = return_var_list(arr_var_sbt)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VIbEyLhXXuEm"},"source":["df = pd.DataFrame(varlist_bdd, columns=['bdd'])\n","df['sbt'] = varlist_sbt\n","df['bdd_input'] = varlist_bdd\n","df['bdd_label'] = varlist_bdd\n","df.tail(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K_7MpCjkXuEm"},"source":["# Calcul du nombre de var max par code et du nombre de lettre total"]},{"cell_type":"code","metadata":{"id":"wG6EpcpRXuEm"},"source":["'''\n","def return_nb_var_maxlen_var(arr):\n","    all_len = []\n","    nb_var = []\n","    for words in arr:\n","        temp_len = 0\n","        for j,word in enumerate(words):\n","            for i,char in enumerate(word):\n","                _\n","            temp_len=temp_len+i+1\n","        nb_var.append(j+1)\n","        all_len.append(temp_len)\n","    return max(nb_var), max(all_len)\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5FEEajEQXuEn"},"source":["# Creation d'un vocabulaire pour les variables "]},{"cell_type":"code","metadata":{"id":"AlU9wGWvXuEn"},"source":["vocab_variables_bdd = {l for char in variables_bdd for l in char}\n","vocab_variables_bdd.add('/')\n","vocab_variables_bdd.add('sostoken ')\n","vocab_variables_bdd.add(' eostoken')\n","char_to_ind_bdd = {char : ind for ind,char in enumerate(vocab_variables_bdd)}\n","ind_to_char_bdd = {ind : char for ind,char in enumerate(vocab_variables_bdd)}\n","#print(char_to_ind_bdd)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YPOUiX3NXuEn"},"source":["vocab_variables_sbt = {l for char in variables_sbt for l in char}\n","vocab_variables_sbt.add('/')\n","char_to_ind_sbt = {char : ind for ind,char in enumerate(vocab_variables_sbt)}\n","ind_to_char_sbt = {ind : char for ind,char in enumerate(vocab_variables_sbt)}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Se5k2t3TXuEn"},"source":["# Valeurs encodage BDD"]},{"cell_type":"code","metadata":{"id":"x1NSNBhfXuEn"},"source":["number_variables_bdd = 13\n","len_chain= 92\n","#number_variables_bdd, len_chain = return_nb_var_maxlen_var(arr_var_bdd)\n","number_char_variables_bdd = len(vocab_variables_bdd)\n","len_total_chain_bdd = number_variables_bdd+len_chain+1\n","'''\n","print('nombre total de bdd :', num_folder)\n","print('nombre total de variables par fichier bdd:', number_variables_bdd)\n","print('taille de la chaine :',len_chain)\n","print('nombre total de char dans la chaine avec / :',len_total_chain_bdd)\n","print('nombre de char differents bdd :', number_char_variables_bdd)\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ERqhxHdnXuEo"},"source":["# Valeur encodage SBT"]},{"cell_type":"code","metadata":{"id":"DtLt1WTzXuEo"},"source":["number_variables_sbt=22\n","len_chain_sbt=50\n","#number_variables_sbt, len_chain_sbt = return_nb_var_maxlen_var(arr_var_sbt)\n","number_char_variables_sbt = len(vocab_variables_sbt)\n","len_total_chain_sbt = number_variables_sbt+len_chain_sbt+1\n","'''\n","print('nombre total de bdd :', num_folder)\n","print('nombre total de variables par fichier sbt:', number_variables_sbt)\n","print('taille de la chaine :',len_chain_sbt)\n","print('nombre total de char dans la chaine avec / :',len_total_chain_sbt)\n","print('nombre de char differents sbt :', number_char_variables_sbt)\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FAkaI3tvXuEo"},"source":["Type d'encodage : les données sont encodées en 'phrases' toutes les variables sont situées sur une seule ligne de tableau et ne sont plus séparées :\n","\n","ex : 'Fonction_1/bdd_Fonction_1_1/z/-6/.../Fonction_1'"]},{"cell_type":"code","metadata":{"id":"xXxWa-WeXuEo"},"source":["\n","encoder_input = np.array(df.sbt)\n","decoder_input = np.array(df.bdd_input)\n","decoder_label = np.array(df.bdd_label)\n","\n","\n","indices = np.arange(num_folder)\n","np.random.shuffle(indices)\n","\n","encoder_input = encoder_input[indices]\n","decoder_input = decoder_input[indices]\n","decoder_label = decoder_label[indices]\n","\n","df.head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4e3tMTDXXuEo"},"source":["total = num_folder\n","test_size = 0.1\n","\n","train_encoder_input = encoder_input[:-int(total*test_size)]\n","train_decoder_input = decoder_input[:-int(total*test_size)]\n","train_decoder_label = decoder_label[:-int(total*test_size)]\n","\n","test_encoder_input = encoder_input[-int(total*test_size):]\n","test_decoder_input = decoder_input[-int(total*test_size):]\n","test_decoder_label = decoder_label[-int(total*test_size):]\n","'''\n","print(\"train dataset shape\")\n","print(train_encoder_input.shape)\n","print(train_decoder_input.shape)\n","print(train_decoder_label.shape)\n","\n","print(\"\\n\\ntest dataset shape\")\n","print(test_encoder_input.shape)\n","print(test_decoder_input.shape)\n","print(test_decoder_label.shape)\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cSfHEbU9XuEp"},"source":["Il faut maintenant convertir ce code en sequences, puis padded ces sequences, on va pas ajouter les tokens de debut et fin maintenant, on les ajoutera apres avoir encodé"]},{"cell_type":"code","metadata":{"id":"fBIXo__yXuEp"},"source":["def encode(variables,char_to_ind, dim1, dim2,type):# type 0 encoder, 1 decoder_input, 2 decoder_label\n","    encoded_variables_matrix = np.full((dim1,dim2),char_to_ind['/'])\n","    #print('Encoding Matrix shape : ',encoded_variables_matrix.shape)\n","    for i,vars in enumerate(variables):\n","        j=0\n","        #print(vars)\n","        if type == 1:\n","            encoded_variables_matrix[i,j]=char_to_ind['sostoken ']\n","            j=j+1\n","        \n","        for var in (vars):\n","            #print(var)\n","            #print(' j :',j)\n","            encoded_variables_matrix[i,j]=char_to_ind[var]\n","\n","            j=j+1\n","        if type == 2:\n","            encoded_variables_matrix[i,j]=char_to_ind[' eostoken']\n","            j=j+1\n","\n","    return encoded_variables_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-EVvmycbXuEp"},"source":["def encode_unique_var(text):\n","    encoded_matrix_unique = np.full(73,char_to_ind_sbt['/'])\n","    for i,char in enumerate(text):\n","        encoded_matrix_unique[i]=char_to_ind_sbt[char]\n","\n","    return encoded_matrix_unique"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2WlgDPzLXuEp"},"source":["#print(\"train dataset shape\")\n","train_encoder_input_encoded = encode(train_encoder_input, char_to_ind_sbt,len(train_encoder_input),len_total_chain_sbt,0)\n","train_decoder_input_encoded = encode(train_decoder_input, char_to_ind_bdd,len(train_decoder_input),len_total_chain_bdd,1)\n","train_decoder_label_encoded = encode(train_decoder_label, char_to_ind_bdd,len(train_decoder_label),len_total_chain_bdd,2)\n","#print(\"\\n\\ntest dataset shape\")\n","\n","test_encoder_input_encoded = encode(test_encoder_input, char_to_ind_sbt,len(test_encoder_input),len_total_chain_sbt,0)\n","test_decoder_input_encoded = encode(test_decoder_input, char_to_ind_bdd,len(test_decoder_input),len_total_chain_bdd,1)\n","test_decoder_label_encoded = encode(test_decoder_label, char_to_ind_bdd,len(test_decoder_label),len_total_chain_bdd,2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AqPJwApQXuEq"},"source":["def decode ( encoded_sequence, ind_to_char):\n","    decoded_sequence = []\n","    for var in encoded_sequence:\n","        decoded_sequence.append(ind_to_char[var])\n","    return ''.join(decoded_sequence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B86p6f9mXuEq"},"source":["decode(train_decoder_label_encoded[0],ind_to_char_bdd)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J251ouViXuEq"},"source":["from keras.layers import Input,Embedding,LSTM,Dense,Concatenate,Attention\n","from keras.models import Model\n","from keras import backend as K\n","\n","#hyperparameters\n","embedding_size = 256\n","hidden_size = 256\n","\n","# trainer model (generator model will use the same encoder tho)\n","encoder_input = Input(shape=[len_total_chain_sbt]) # size chain SBT \n","encoder_embedding = Embedding(len_total_chain_sbt,embedding_size,mask_zero=True)\n","encoder_embedded = encoder_embedding(encoder_input)\n","\n","encoder_lstm1 = LSTM(hidden_size,return_sequences=True,return_state=True,dropout=0.2,recurrent_dropout=0.2)\n","encoder_output1,encoder_h1,encoder_c1 = encoder_lstm1(encoder_embedded)\n","\n","encoder_lstm2 = LSTM(hidden_size,return_sequences=True,return_state=True,dropout=0.2,recurrent_dropout=0.2)\n","encoder_output2,encoder_h2,encoder_c2 = encoder_lstm2(encoder_output1)\n","\n","encoder_lstm3 = LSTM(hidden_size,return_sequences=True,return_state=True,dropout=0.2,recurrent_dropout=0.2)\n","encoder_output3,encoder_h3,encoder_c3 = encoder_lstm3(encoder_output1)\n","\n","decoder_input = Input(shape=(None,))\n","decoder_embedding = Embedding(len_total_chain_bdd,embedding_size,mask_zero=True) # len chain bdd\n","decoder_embedded = decoder_embedding(decoder_input)\n","\n","decoder_lstm = LSTM(hidden_size,return_sequences=True,return_state=True,dropout=0.2,recurrent_dropout=0.2)\n","decoder_output,_,_ = decoder_lstm(decoder_embedded,initial_state=[encoder_h3,encoder_c3])\n","\n","attn_layer = Attention()\n","attn_context = attn_layer([decoder_output,encoder_output3])\n","\n","decoder_output = Concatenate(axis=-1)([decoder_output,attn_context])\n","tanh_dense= Dense(hidden_size*2,activation=K.tanh)\n","decoder_output = tanh_dense(decoder_output)\n","\n","softmax_dense = Dense(number_char_variables_bdd,activation='softmax')\n","decoder_output = softmax_dense(decoder_output)\n","\n","trainer_model = Model([encoder_input,decoder_input],decoder_output)\n","trainer_model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8hKP_82CXuEq"},"source":["trainer_hist =trainer_model.fit([train_encoder_input_encoded,train_decoder_input_encoded],train_decoder_label_encoded,epochs=40,batch_size=128,validation_split=0.2, verbose=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mPxF7J7fXuEr"},"source":["losses = pd.DataFrame(trainer_model.history.history)\n","losses.plot()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AONIwsbaXuEr"},"source":["score = trainer_model.evaluate([train_encoder_input_encoded,train_decoder_input_encoded], train_decoder_label_encoded, verbose=0)\n","print(\"%s: %.2f%%\" % (trainer_model.metrics_names[1], score[1]*100))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wsso50VNXuEr"},"source":["#trainer_model.load_weights('attention_variables_weights.h5')\n","#trainer_model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n","trainer_model.save_weights('last.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EhDaxBy_XuEr"},"source":["#generator model\n","gen_encoder = Model([encoder_input],[encoder_output3,encoder_h3,encoder_c3])\n","\n","gen_decoder_values_input = Input(shape=(len_total_chain_sbt,hidden_size))\n","gen_decoder_h_input = Input(shape=[hidden_size])\n","gen_decoder_c_input = Input(shape=[hidden_size])\n","\n","gen_decoder_embedded = decoder_embedding(decoder_input)\n","gen_decoder_output,gen_decoder_h,gen_decoder_c = decoder_lstm(gen_decoder_embedded,initial_state=[gen_decoder_h_input,gen_decoder_c_input])\n","\n","attn_context = attn_layer([gen_decoder_output,gen_decoder_values_input])\n","gen_decoder_output = Concatenate(axis=-1)([gen_decoder_output,attn_context])\n","\n","gen_decoder_output = tanh_dense(gen_decoder_output)\n","gen_decoder_output = softmax_dense(gen_decoder_output)\n","\n","gen_decoder = Model([decoder_input]+[gen_decoder_values_input,gen_decoder_h_input,gen_decoder_c_input],[gen_decoder_output,gen_decoder_h,gen_decoder_c])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s0plOleopAFc"},"source":["# Test"]},{"cell_type":"code","metadata":{"id":"87et-Cq2XuEr"},"source":["def generate_from_encoder_input(encoder_input):\n","    encoder_input = encoder_input.reshape(1,-1)\n","    values,h,c = gen_encoder.predict(encoder_input)\n","    \n","    single_tok = np.zeros((1,1))\n","    single_tok[0,0] = char_to_ind_bdd['sostoken ']\n","    decoder_input = single_tok\n","    \n","    generated = []\n","    count = 0\n","    while(True):\n","        decoder_output,new_h,new_c = gen_decoder.predict([decoder_input]+[values,h,c])\n","        count +=1\n","        \n","        sampled_index = np.argmax(decoder_output[0,-1,:])\n","        sampled_word = ind_to_char_bdd[sampled_index]\n","        \n","        if sampled_word != ' eostoken' and sampled_index != 0:\n","            generated.append(sampled_word)\n","        if count >= len_total_chain_bdd or sampled_word == ' eostoken':\n","            break\n","        \n","        h,c = new_h,new_c\n","        decoder_input[0,0] = sampled_index\n","    \n","    generated = ''.join(generated)\n","    return generated"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"euWdmQI4XuEs"},"source":["'''\n","for i in range(520,525):\n","    print(\"\\n<<sample encoder input SBT >>\")\n","    print(decode(train_encoder_input_encoded[i],ind_to_char_sbt))\n","    print(\"\\n\")\n","    print(\"<<generated BDD >>\")\n","    print(generate_from_encoder_input(train_encoder_input_encoded[i]))\n","    print(\"\\n\")\n","    print(\"<<Expected BDD>>\")\n","    print(decode(train_decoder_label_encoded[i],ind_to_char_bdd))\n","    print(\"========================================\\n\")\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iA0lrG28XuEs"},"source":["def return_generated_var(input_encoded):\n","    generated = generate_from_encoder_input(input_encoded)\n","    generated = generated.split('/')\n","    return generated"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NDPMrHbqXuEs"},"source":["#print(generate_from_encoder_input(train_encoder_input_encoded[0]))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nna0s1dhXuEs"},"source":["return_generated_var(train_encoder_input_encoded[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CYmabjQWXuEs"},"source":[""],"execution_count":null,"outputs":[]}]}