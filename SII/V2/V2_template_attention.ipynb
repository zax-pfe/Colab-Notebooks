{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"V2_template_attention.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"cells":[{"cell_type":"code","metadata":{"id":"FpeH-aYoa2DB"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import tensorflow as tf\n","import os\n","from collections import Counter\n","\n","##############################\n","# Definition of Env variable # \n","##############################\n","#Definition of the current env\n","# JUPYTER or G_COLAB\n","CURRENT_ENV = \"G_COLAB\"\n","\n","#Use on Jupyter_env only \n","DATA_DIR = \"../../dev_GenJavaBdd/GenerationTest/\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EYp9_VqacjWN"},"source":["# Lecture des fichiers\n","get the number of sbt/bdd function"]},{"cell_type":"code","metadata":{"id":"6FKj9y4obUDu"},"source":["if CURRENT_ENV == \"JUPYTER\":\n","    list_folder = os.listdir(DATA_DIR)\n","    num_folder = len(list_folder)-2\n","elif CURRENT_ENV == \"G_COLAB\":\n","    with open(\"bdd.txt\",\"r\") as f:\n","        num_folder = len(f.readlines())\n","    with open(\"sbt.txt\",\"r\") as f:\n","        if (len(f.readlines()) != num_folder):\n","            error(\"[ERROR] number of BDD function does not match BDD number\")\n","print(\"Number of function/Foler = \", num_folder)  # This would give length of files.\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4dzz9w1icmD8"},"source":["# Creation des listes "]},{"cell_type":"code","metadata":{"id":"QSbrQT_iXs8N"},"source":["from tqdm import tqdm\n","from time import sleep"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Gq7uxm3cybc"},"source":["bdd = []\n","sbt = []\n","\n","#Create the list and save it as file\n","if CURRENT_ENV == \"JUPYTER\":\n","    print(\"Get the bdd and sbt list via folder\")\n","    \n","    # Sleep to prevent fail tqdm print\n","    #sleep(1)\n","    #for i  in tqdm (range(num_folder), desc=\"loading...\"):\n","    for i  in range(num_folder):\n","        path_iteration = DATA_DIR+'/Dir_Fonction_'+str(i+1)\n","        bdd_path = path_iteration+\"/BDD_Fonction_\"+str(i+1)+\".txt\"\n","        stb_path = path_iteration+\"/SBT_Generation_Output/Fonction_\"+str(i+1)+\"_SBT.txt\"\n","        bdd_txt = open(bdd_path,'r').read()\n","        sbt_txt = open(stb_path,'r').read()\n","        clean_bdd = cleaning_bdd(bdd_txt)\n","        bdd.append(clean_bdd)\n","        sbt.append(sbt_txt)\n","    #sleep(1)\n","    \n","    print(\"Save the BDD & SBT list as file\")\n","    # Save the list as file\n","    #os.remove(\"bddi.txt\")\n","    f_bdd = open(\"bdd.txt\", \"w+\")\n","    f_bdd.write('\\n'.join(bdd))\n","    f_bdd.close()\n","\n","    #os.remove(\"sbt.txt\")\n","    f_sbt = open(\"sbt.txt\", \"w+\")\n","    f_sbt.write('\\n'.join(sbt))\n","    f_sbt.close()\n","    \n","#Create the list from the txt file\n","else:\n","    print(\"Get the bdd and sbt list via the bdd.txt & sbt.txt\")\n","    with open(\"bdd.txt\") as f:\n","        tmp_bdd = f.read()\n","    bdd = tmp_bdd.split(\"\\n\")\n","\n","    with open(\"sbt.txt\") as f:\n","        tmp_sbt = f.read()\n","    sbt = tmp_sbt.split(\"\\n\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hi8C66HgXs8O"},"source":["# Cette fonction sera utilisée pour récupérer les codes sbt et bdd dans un autre code python\n","def return_bdd_sbt():\n","    return bdd,sbt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fj1ndvwJXs8O"},"source":["bdd2,_ = return_bdd_sbt()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SpMQwRC7Xs8O"},"source":["# Recuperation des valeurs max de bdd et sbt"]},{"cell_type":"code","metadata":{"id":"-i6MVpKqXs8O"},"source":["def return_max_len( text ):\n","    all_len = []\n","    for unique_code in text:\n","        code_split = unique_code.split(' ')\n","        all_len.append(len(code_split))\n","    return max(all_len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S-DzPXc3Xs8P"},"source":["bdd_counter =Counter([word for sentence in bdd for word in sentence.split(' ')])\n","sbt_counter =Counter([word for sentence in sbt for word in sentence.split(' ')])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BYk3hMdIXs8P"},"source":["def return_common_words(counter):\n","    common_words = []\n","    for word in counter:\n","        occurence = counter[word]\n","        if occurence > 3500:\n","            common_words.append(word)\n","    return common_words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y71-EjT3Xs8P"},"source":["common_words_bdd = return_common_words(bdd_counter)\n","common_words_sbt = return_common_words(sbt_counter)\n","\n","common_words_bdd.append('<oov>')\n","common_words_sbt.append('<oov>')\n","common_words_bdd.append('sostoken ')\n","common_words_bdd.append(' eostoken')\n","common_words_bdd.append(' ')\n","common_words_sbt.append(' ')\n","\n","#print('Common words SBT : \\n',common_words_sbt)\n","#print('\\n')\n","#print('Common words BDD : \\n',common_words_bdd)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eUZumIIbXs8P"},"source":["word_to_ind_sbt = {word : ind for ind,word in enumerate(common_words_sbt)}\n","ind_to_word_sbt = {ind : word for ind,word in enumerate(common_words_sbt)}\n","\n","word_to_ind_bdd = {word : ind for ind,word in enumerate(common_words_bdd)}\n","ind_to_word_bdd = {ind : word for ind,word in enumerate(common_words_bdd)}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2LkZNHLYXs8Q"},"source":["number_folder = len(bdd)\n","max_words_sbt = return_max_len(sbt)+1\n","max_words_bdd = return_max_len(bdd)+1\n","number_of_words_sbt = len(common_words_sbt)\n","number_of_words_bdd = len(common_words_bdd)\n","'''\n","print(' number of forlder :',number_folder)\n","print(' number maximum of word in one sbt code :',max_words_sbt)\n","print(' number maximum of word in one bdd code :',max_words_bdd)\n","print(' number of different word in sbt :',number_of_words_sbt)\n","print(' number of different word in bdd :',number_of_words_bdd)\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2Cj5y-EAXs8Q"},"source":["# ---------------------------------------------------------------------------------------------------------------\n","\n","# Encodage BDD"]},{"cell_type":"code","metadata":{"id":"0lgDoabPXs8Q"},"source":["def return_listing(text,common_words): \n","    arr= [[] for _ in range(num_folder)] #pour calculer la taille max de toute les variables réunies \n","    var = [] # pour creer un vocabulaire \n","    for i,sentence in  enumerate(text):\n","        text_split = sentence.split( ' ')\n","        for j,word in enumerate (text_split):\n","            if word not in common_words:\n","                arr[i].append(word)\n","                var.append(word) \n","    return arr, var"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wkQhRUqKXs8R"},"source":["arr_var_bdd, variables_bdd = return_listing(bdd,common_words_bdd)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dypgxK4jXs8R"},"source":["arr_var_sbt,variables_sbt= return_listing(sbt,common_words_sbt)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"64Fbu8VIXs8R"},"source":["Conversion des variables en sentences, puis on transforme ce tablea de sentences en dataframe pour pouvoir appliquer les sostoken et eosteoken"]},{"cell_type":"code","metadata":{"id":"Ng2PBTjuXs8R"},"source":["def return_txt_trous(text,common_words):\n","    \n","    txt_trous = []\n","    \n","    \n","    for i,words in enumerate(text):\n","        temp_sentence = []\n","        split_words = words.split(' ')\n","        for j,word in enumerate(split_words):\n","            if word not in common_words:\n","                temp_sentence.append('<oov>')\n","                \n","            if word in common_words:\n","                temp_sentence.append(word)\n","                            \n","        txt_trous.append(' '.join(temp_sentence))\n","    return txt_trous"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FHXeabzeXs8S"},"source":["def return_txt_trous_unique(text):\n","    \n","    txt_trous = []  \n","    split_words = text.split(' ')\n","    for j,word in enumerate(split_words):\n","        if word not in common_words_sbt:\n","            txt_trous.append('<oov>')\n","\n","        if word in common_words_sbt:\n","            txt_trous.append(word)\n","\n","    return ' '.join(txt_trous)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mPeLwtduXs8S"},"source":["txt_trou_unique = return_txt_trous_unique(sbt[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QvdFhXNNXs8S"},"source":["txt_trou_bdd = return_txt_trous(bdd, common_words_bdd)\n","txt_trou_sbt = return_txt_trous(sbt, common_words_sbt)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pROcJHfnXs8S"},"source":["df = pd.DataFrame(txt_trou_bdd, columns=['bdd'])\n","df['sbt'] = txt_trou_sbt\n","df['bdd_input'] = txt_trou_bdd\n","df['bdd_label'] = txt_trou_bdd\n","df.tail(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5O0zUVKaXs8S"},"source":["# Calcul du nombre de var max par code et du nombre de lettre total"]},{"cell_type":"code","metadata":{"id":"T0n9UhjkXs8T"},"source":["def return_nb_var_maxlen_var(arr):\n","    all_len = []\n","    nb_var = []\n","    for words in arr:\n","        temp_len = 0\n","        for j,word in enumerate(words):\n","            for i,char in enumerate(word):\n","                _\n","            temp_len=temp_len+i+1\n","        nb_var.append(j+1)\n","        all_len.append(temp_len)\n","    return max(nb_var), max(all_len)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ArDuE7xXXs8T"},"source":["# Creation d'un vocabulaire pour les variables "]},{"cell_type":"code","metadata":{"id":"skwB64HoXs8T"},"source":["\n","encoder_input = np.array(df.sbt)\n","decoder_input = np.array(df.bdd_input)\n","decoder_label = np.array(df.bdd_label)\n","\n","\n","indices = np.arange(num_folder)\n","np.random.shuffle(indices)\n","\n","encoder_input = encoder_input[indices]\n","decoder_input = decoder_input[indices]\n","decoder_label = decoder_label[indices]\n","\n","df.head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bnDHVLiYXs8T"},"source":["total = num_folder\n","test_size = 0.1\n","\n","train_encoder_input = encoder_input[:-int(total*test_size)]\n","train_decoder_input = decoder_input[:-int(total*test_size)]\n","train_decoder_label = decoder_label[:-int(total*test_size)]\n","\n","test_encoder_input = encoder_input[-int(total*test_size):]\n","test_decoder_input = decoder_input[-int(total*test_size):]\n","test_decoder_label = decoder_label[-int(total*test_size):]\n","'''\n","print(\"train dataset shape\")\n","print(train_encoder_input.shape)\n","print(train_decoder_input.shape)\n","print(train_decoder_label.shape)\n","\n","print(\"\\n\\ntest dataset shape\")\n","print(test_encoder_input.shape)\n","print(test_decoder_input.shape)\n","print(test_decoder_label.shape)\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DfcHuaxPXs8T"},"source":["Il faut maintenant convertir ce code en sequences, puis padded ces sequences, on va pas ajouter les tokens de debut et fin maintenant, on les ajoutera apres avoir encodé"]},{"cell_type":"code","metadata":{"id":"ox-FGHPxXs8U"},"source":["def encode(text,word_to_ind, dim1, dim2,type):# type 0 encoder, 1 decoder_input, 2 decoder_label\n","    encoded_matrix = np.full((dim1,dim2),word_to_ind[' '])\n","    #print('Encoding Matrix shape : ',encoded_matrix.shape)\n","    for i,words in enumerate(text):\n","        j=0\n","        #print(vars)\n","        split_word = words.split(' ')\n","        if type == 1:\n","            encoded_matrix[i,j]=word_to_ind['sostoken ']\n","            j=j+1\n","        \n","        for word in (split_word):\n","            encoded_matrix[i,j]=word_to_ind[word]\n","\n","            j=j+1\n","        if type == 2:\n","            encoded_matrix[i,j]=word_to_ind[' eostoken']\n","            j=j+1\n","\n","    return encoded_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QWW6_9ZiXs8U"},"source":["def encode_unique(text):# type 0 encoder, 1 decoder_input, 2 decoder_label\n","    encoded_matrix_unique = np.full(256,word_to_ind_sbt[' '])\n","\n","    split_word = text.split(' ')\n","\n","    for i,word in enumerate(split_word):\n","        encoded_matrix_unique[i]=word_to_ind_sbt[word]\n","\n","    return encoded_matrix_unique"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_lxOrhdsXs8U"},"source":["txt_trou_unique"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F-l03HdBXs8U"},"source":["encode_unique(txt_trou_unique)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LY9ZVLPxXs8V"},"source":["#print(\"train dataset shape\")\n","train_encoder_input_encoded = encode(train_encoder_input, word_to_ind_sbt,len(train_encoder_input),max_words_sbt,0)\n","train_decoder_input_encoded = encode(train_decoder_input, word_to_ind_bdd,len(train_decoder_input),max_words_bdd,1)\n","train_decoder_label_encoded = encode(train_decoder_label, word_to_ind_bdd,len(train_decoder_label),max_words_bdd,2)\n","#print(\"\\n\\ntest dataset shape\")\n","\n","test_encoder_input_encoded = encode(test_encoder_input, word_to_ind_sbt,len(test_encoder_input),max_words_sbt,0)\n","test_decoder_input_encoded = encode(test_decoder_input, word_to_ind_bdd,len(test_decoder_input),max_words_bdd,1)\n","test_decoder_label_encoded = encode(test_decoder_label, word_to_ind_bdd,len(test_decoder_label),max_words_bdd,2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rP6ZEiR8Xs8V"},"source":["train_encoder_input_encoded.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zb3FUYkgXs8V"},"source":["def decode ( encoded_sequence, ind_to_char):\n","    decoded_sequence = []\n","    for var in encoded_sequence:\n","        decoded_sequence.append(ind_to_char[var])\n","    return ' '.join(decoded_sequence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xm4_RzC9Xs8V"},"source":["decode(train_decoder_label_encoded[0],ind_to_word_bdd)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lO9dze8wXs8V"},"source":["from keras.layers import Input,Embedding,LSTM,Dense,Concatenate,Attention\n","from keras.models import Model\n","#from keras.utils import plot_model\n","from keras import backend as K\n","\n","#hyperparameters\n","embedding_size = 256\n","hidden_size = 256\n","\n","# trainer model (generator model will use the same encoder tho)\n","encoder_input = Input(shape=[max_words_sbt]) # size chain SBT \n","encoder_embedding = Embedding(max_words_sbt,embedding_size,mask_zero=True)\n","encoder_embedded = encoder_embedding(encoder_input)\n","\n","encoder_lstm1 = LSTM(hidden_size,return_sequences=True,return_state=True,dropout=0.2,recurrent_dropout=0.2)\n","encoder_output1,encoder_h1,encoder_c1 = encoder_lstm1(encoder_embedded)\n","\n","encoder_lstm2 = LSTM(hidden_size,return_sequences=True,return_state=True,dropout=0.2,recurrent_dropout=0.2)\n","encoder_output2,encoder_h2,encoder_c2 = encoder_lstm2(encoder_output1)\n","\n","encoder_lstm3 = LSTM(hidden_size,return_sequences=True,return_state=True,dropout=0.2,recurrent_dropout=0.2)\n","encoder_output3,encoder_h3,encoder_c3 = encoder_lstm3(encoder_output1)\n","\n","decoder_input = Input(shape=(None,))\n","decoder_embedding = Embedding(max_words_bdd,embedding_size,mask_zero=True) # len chain bdd\n","decoder_embedded = decoder_embedding(decoder_input)\n","\n","decoder_lstm = LSTM(hidden_size,return_sequences=True,return_state=True,dropout=0.2,recurrent_dropout=0.2)\n","decoder_output,_,_ = decoder_lstm(decoder_embedded,initial_state=[encoder_h3,encoder_c3])\n","\n","attn_layer = Attention()\n","attn_context = attn_layer([decoder_output,encoder_output3])\n","\n","decoder_output = Concatenate(axis=-1)([decoder_output,attn_context])\n","tanh_dense= Dense(hidden_size*2,activation=K.tanh)\n","decoder_output = tanh_dense(decoder_output)\n","\n","softmax_dense = Dense(number_of_words_bdd,activation='softmax')\n","decoder_output = softmax_dense(decoder_output)\n","\n","trainer_model = Model([encoder_input,decoder_input],decoder_output)\n","trainer_model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7EiR1BOSXs8W"},"source":["#trainer_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VsZI3epHXs8W"},"source":["#trainer_hist =trainer_model.fit([train_encoder_input_encoded,train_decoder_input_encoded],train_decoder_label_encoded,epochs=5,batch_size=128,validation_split=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t4UkefmVXs8W"},"source":["#trainer_model.save_weights('template_attention.h5')\n","trainer_model.load_weights('template_attention.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PzILhyInXs8W"},"source":["#generator model\n","gen_encoder = Model([encoder_input],[encoder_output3,encoder_h3,encoder_c3])\n","\n","gen_decoder_values_input = Input(shape=(max_words_sbt,hidden_size))\n","gen_decoder_h_input = Input(shape=[hidden_size])\n","gen_decoder_c_input = Input(shape=[hidden_size])\n","\n","gen_decoder_embedded = decoder_embedding(decoder_input)\n","gen_decoder_output,gen_decoder_h,gen_decoder_c = decoder_lstm(gen_decoder_embedded,initial_state=[gen_decoder_h_input,gen_decoder_c_input])\n","\n","attn_context = attn_layer([gen_decoder_output,gen_decoder_values_input])\n","gen_decoder_output = Concatenate(axis=-1)([gen_decoder_output,attn_context])\n","\n","gen_decoder_output = tanh_dense(gen_decoder_output)\n","gen_decoder_output = softmax_dense(gen_decoder_output)\n","\n","gen_decoder = Model([decoder_input]+[gen_decoder_values_input,gen_decoder_h_input,gen_decoder_c_input],[gen_decoder_output,gen_decoder_h,gen_decoder_c])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s0plOleopAFc"},"source":["# Test"]},{"cell_type":"code","metadata":{"id":"0--tf2mMXs8X"},"source":["def generate_from_encoder_input(encoder_input):\n","    encoder_input = encoder_input.reshape(1,-1)\n","    values,h,c = gen_encoder.predict(encoder_input)\n","    \n","    single_tok = np.zeros((1,1))\n","    single_tok[0,0] = word_to_ind_bdd['sostoken ']\n","    decoder_input = single_tok\n","    \n","    generated = []\n","    count = 0\n","    while(True):\n","        decoder_output,new_h,new_c = gen_decoder.predict([decoder_input]+[values,h,c])\n","        count +=1\n","        \n","        sampled_index = np.argmax(decoder_output[0,-1,:])\n","        sampled_word = ind_to_word_bdd[sampled_index]\n","        \n","        if sampled_word != ' eostoken' and sampled_index != 0:\n","            generated.append(sampled_word)\n","        if count >= max_words_bdd or sampled_word == ' eostoken':\n","            break\n","        \n","        h,c = new_h,new_c\n","        decoder_input[0,0] = sampled_index\n","    \n","    generated = ' '.join(generated)\n","    return generated"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8b75_dIIXs8X"},"source":["'''\n","for i in range(520,525):\n","    print(\"\\n<<sample encoder input SBT >>\")\n","    print(decode(train_encoder_input_encoded[i],ind_to_word_sbt))\n","    print(\"\\n\")\n","    print(\"<<generated BDD >>\")\n","    print(generate_from_encoder_input(train_encoder_input_encoded[i]))\n","    print(\"\\n\")\n","    print(\"<<Expected BDD>>\")\n","    print(decode(train_decoder_label_encoded[i],ind_to_word_bdd))\n","    print(\"========================================\\n\")\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7rMv_nqnXs8X"},"source":["input_encoded = test_encoder_input_encoded[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bLvYMdcIXs8X"},"source":["def return_generated_template(input_encoded):\n","    generated = generate_from_encoder_input(input_encoded)\n","    generated = generated.split(' ')\n","    return generated"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sQQKDbWfXs8X"},"source":["score = trainer_model.evaluate([train_encoder_input_encoded,train_decoder_input_encoded], train_decoder_label_encoded, verbose=0)\n","print(\"%s: %.2f%%\" % (trainer_model.metrics_names[1], score[1]*100))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XIV4w5cyXs8X"},"source":[""],"execution_count":null,"outputs":[]}]}