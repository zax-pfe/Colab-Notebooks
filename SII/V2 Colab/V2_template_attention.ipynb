{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"V2_template_attention.ipynb","provenance":[],"collapsed_sections":["dsjLsYJ1Z6T2","FUCVGZK_Z6T6"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"code","metadata":{"id":"uKmgTepqZ6Tu"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import tensorflow as tf\n","import os\n","from collections import Counter"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EEigbCvxZ6Tv"},"source":["from tqdm import tqdm # permet d'afficher des barres de chargement "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6SfMsyQ4Z6Tw","colab":{"base_uri":"https://localhost:8080/","height":229},"executionInfo":{"status":"error","timestamp":1630571947028,"user_tz":-120,"elapsed":394,"user":{"displayName":"Axel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSgb-gUyVm7Dftsq1qnzVUodf0QJgXJ6Q7_nvF1g=s64","userId":"01637125792908796850"}},"outputId":"775c0240-8e91-415a-c33e-3f84309733e9"},"source":["with open(\"bdd.txt\",\"r\") as f:\n","    num_folder = len(f.readlines())\n","with open(\"sbt.txt\",\"r\") as f:\n","    if (len(f.readlines()) != num_folder):\n","        error(\"[ERROR] number of BDD function does not match BDD number\")\n","print(\"Number of function/Foler = \", num_folder)  # This would give length of files."],"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-a990fa8c5e7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bdd.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mnum_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sbt.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnum_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[ERROR] number of BDD function does not match BDD number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'bdd.txt'"]}]},{"cell_type":"code","metadata":{"id":"8Gq7uxm3cybc"},"source":["print(\"Get the bdd and sbt list via the bdd.txt & sbt.txt\")\n","with open(\"bdd.txt\") as f:\n","    tmp_bdd = f.read()\n","bdd = tmp_bdd.split(\"\\n\")\n","\n","with open(\"sbt.txt\") as f:\n","    tmp_sbt = f.read()\n","sbt = tmp_sbt.split(\"\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XdNaguOEZ6Tx"},"source":["# Cette fonction sera utilisée pour récupérer les codes sbt et bdd dans un autre code python\n","def return_bdd_sbt_and_num_folder():\n","    \n","    return bdd,sbt, num_folder"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uw0iEtSTZ6Ty"},"source":["# Recuperation des valeurs max de BDD et SBT\n","\n","Afin de creer une matrice d'encodage, il faut dans un premier temps determiner quelles seront les dimensions de cette matrice. Comme nous encodons en integer encoding les dimmensions des matrices vont etre les suivantes : ( nombre de fichiers, nombre de mot max par fichier ) "]},{"cell_type":"markdown","metadata":{"id":"dJ9ewlnAZ6Ty"},"source":["La cellule ci dessous retourne le fichier ayant la plus grande longueur, donc qui possede le plus de mot :"]},{"cell_type":"code","metadata":{"id":"6fqs7MYrZ6Ty"},"source":["#Cette fonction calcule la taille de chaque code et renvoit le taille maximale. \n","def return_max_len( text ):\n","    all_len = []\n","    for unique_code in text:\n","        code_split = unique_code.split(' ')\n","        all_len.append(len(code_split))\n","    return max(all_len)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3PFwKe4HZ6Tz"},"source":["# Separation variables / template "]},{"cell_type":"markdown","metadata":{"id":"o3RZZ2DuZ6Tz"},"source":["Les counters permettent de compter le nombre d'occurence de chaque mot, ces données sont ensuite utilisé pour determiner si un mot appartient a la template ou si c est une variable."]},{"cell_type":"code","metadata":{"id":"VHILEr_SZ6Tz"},"source":["bdd_counter =Counter([word for sentence in bdd for word in sentence.split(' ')])\n","sbt_counter =Counter([word for sentence in sbt for word in sentence.split(' ')])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BbXYtKhxZ6Tz"},"source":["Une fois le nombre d'occurence de chaque mot calculé, il faut ensuite regarder quels mots sont les plus souvent répété. J'ai considéré que sur 5000 fichiers, si un mot apparait plus de 3500 fois, alors nous pouvons le considerer comme appartenant a la template. "]},{"cell_type":"code","metadata":{"id":"ogC9nh-mZ6T0"},"source":["'''\n","#Cette fonction retourne une liste contenant les mots les plus rencontrés\n","def return_common_words(counter):\n","    common_words = []\n","    for word in counter:\n","        occurence = counter[word]\n","        if occurence > 3500:\n","            common_words.append(word)\n","    return common_words\n","    \n","    \n","common_words_bdd = return_common_words(bdd_counter)\n","common_words_sbt = return_common_words(sbt_counter)\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t_iZN7QrZ6T0"},"source":["Nous allons utiliser des token speciaux pour remplacer les variables, pour determiner le debut et la fin des sequences. "]},{"cell_type":"code","metadata":{"id":"dbAiYDRRZ6T0"},"source":["common_words_sbt=['', '(', 'MethodDeclaration', 'Modifier', 'public', ')',\n","                  'static', 'SimpleName', 'Parameter', 'PrimitiveType', 'int', 'boolean', 'BlockStmt',\n","                  'ExpressionStmt', 'VariableDeclarationExpr', 'VariableDeclarator', 'IntegerLiteralExpr',\n","                  'res', 'BinaryExpr', 'NameExpr', 'IfStmt', 'ReturnStmt', 'BooleanLiteralExpr', 'true',\n","                  'false', 'UnaryExpr']\n","common_words_bdd =  ['Feature:', 'Check', 'the', '', '#', 'result', 'to', 'True', 'Scenario:', 'Given', '=',\n","                     'and', 'When', 'i', 'call', 'Then', 'should', 'be', 'False']\n","\n","common_words_bdd.append('<oov>')\n","common_words_sbt.append('<oov>')\n","common_words_bdd.append('sostoken ')\n","common_words_bdd.append(' eostoken')\n","common_words_bdd.append(' ')\n","common_words_sbt.append(' ')\n","'''\n","print('Common words SBT : \\n',common_words_sbt)\n","print('\\n')\n","print('Common words BDD : \\n',common_words_bdd)\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LzgPWmLYZ6T1"},"source":["Nous allons maintenant créer des dictionnnaires, c'est à dire associer à chaque mot un nombre. La cellule suivante creer deux dictionnaire pour chaque type de code ( BDD,SBT) qui nous premettent avec un mot d'acceder à son index et inversement."]},{"cell_type":"code","metadata":{"id":"70YL6Jc5Z6T1"},"source":["word_to_ind_sbt = {word : ind for ind,word in enumerate(common_words_sbt)}\n","ind_to_word_sbt = {ind : word for ind,word in enumerate(common_words_sbt)}\n","\n","word_to_ind_bdd = {word : ind for ind,word in enumerate(common_words_bdd)}\n","ind_to_word_bdd = {ind : word for ind,word in enumerate(common_words_bdd)}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pi90BAldZ6T1"},"source":["Afin d'encoder le BDD et SBT il faut creer des matrices, la cellule ci dessous permet de repertorier des données importantes pour leur création."]},{"cell_type":"code","metadata":{"id":"xloXDuMLZ6T1"},"source":["number_folder = len(bdd)\n","max_words_sbt = return_max_len(sbt)+1\n","max_words_bdd = return_max_len(bdd)+1\n","number_of_words_sbt = len(common_words_sbt)\n","number_of_words_bdd = len(common_words_bdd)\n","'''\n","print(' number of forlder :',number_folder)\n","print(' number maximum of word in one sbt code :',max_words_sbt)\n","print(' number maximum of word in one bdd code :',max_words_bdd)\n","print(' number of different word in sbt :',number_of_words_sbt)\n","print(' number of different word in bdd :',number_of_words_bdd)\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dsjLsYJ1Z6T2"},"source":["# ---------------------------------------------------------------------------------------------------------------\n","# Génération des textes à trous"]},{"cell_type":"markdown","metadata":{"id":"k6WP2erRZ6T2"},"source":["La fonction suivante permet de convertir un code SBT  ou BDD en un texte a trous, ou toutes les variables sont remplacées par des tokens ' < oov > ' ( out of vocabulary ) :"]},{"cell_type":"code","metadata":{"id":"i6ULNwSIZ6T2"},"source":["def return_txt_trous(text,common_words):\n","    \n","    txt_trous = []\n","    \n","    \n","    for i,words in enumerate(text):\n","        temp_sentence = []\n","        split_words = words.split(' ')\n","        for j,word in enumerate(split_words):\n","            if word not in common_words:\n","                temp_sentence.append('<oov>')\n","                \n","            if word in common_words:\n","                temp_sentence.append(word)\n","                            \n","        txt_trous.append(' '.join(temp_sentence))\n","    return txt_trous"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JcaIvv-IZ6T3"},"source":["La focntion suivante permet de renvoyer un texte à trous mais pour un unique code SBT :"]},{"cell_type":"code","metadata":{"id":"eDg4iL6iZ6T3"},"source":["def return_txt_trous_unique(text):\n","    \n","    txt_trous = []  \n","    split_words = text.split(' ')\n","    for j,word in enumerate(split_words):\n","        if word not in common_words_sbt:\n","            txt_trous.append('<oov>')\n","\n","        if word in common_words_sbt:\n","            txt_trous.append(word)\n","\n","    return ' '.join(txt_trous)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ud3_qolZZ6T3"},"source":["On génere les textes à trous avec les fonction précedentes"]},{"cell_type":"code","metadata":{"id":"UCH25l7PZ6T3"},"source":["txt_trou_unique = return_txt_trous_unique(sbt[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tCohj8hvZ6T4"},"source":["txt_trou_bdd = return_txt_trous(bdd, common_words_bdd)\n","txt_trou_sbt = return_txt_trous(sbt, common_words_sbt)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pS6gjpPXZ6T4"},"source":["On convertit les textes à trous en dataframe, nous allons ensuite leur appliquer des transformations.\n","Nous créons en plus deux nouvelles colonne dans notre dataframe, bdd_input et bdd_label. "]},{"cell_type":"markdown","metadata":{"id":"bCO--630Z6T4"},"source":["Nous allons donc donner au modele pour son entrainement en entrée le SBT et BDD_input, et en sortie le BDD_label que nous allons dans un premier temps convertir en numpy array"]},{"cell_type":"markdown","metadata":{"id":"sq2Yyt7iZ6T4"},"source":["![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"_wKre2lYZ6T4"},"source":["le fonctionnement d'un tel model est le suivant : \n","Dans un premier temps les informations contenues dans le SBT sont condensée et transformée en un vecteur par l'encoder, ce vecteur est appelé le context vector. \n","\n","Le réseau de neuronne récurent qui prend le BDD_input en entrée et le BDD_Label en sortie, nous permet d'indiquer au modele qu'a la suite d'un certain mot, par exemple \"je\", il doit prédire \"suis\".\n","\n","La couche d'attention permet au modele de se focaliser sur certaines partie de la sequence en entrée afin de prédire par exemple le début de la séquence en sortie.\n","\n","En superposant ces information, le modele est donc capable de savoir que pour un contexte \" i am a student\" et a la suite de \"< s >\" il doit prédire \" Je \" \n","\n","< s > représente notre start of sentence token : sostoken"]},{"cell_type":"code","metadata":{"id":"8M0orS3GZ6T4"},"source":["df = pd.DataFrame(txt_trou_bdd, columns=['bdd'])\n","df['sbt'] = txt_trou_sbt\n","# pour l'instant bdd label et bdd inout sont identiques, nous allons ensuite rajouter les <sostoken> et les <eostoken>\n","df['bdd_input'] = txt_trou_bdd\n","df['bdd_label'] = txt_trou_bdd\n","df.tail(5) #permet d'afficher les 5 derniers éléments"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LUlIgJZEZ6T5"},"source":["Il faut maintenant convertir nos données en numpy array"]},{"cell_type":"code","metadata":{"id":"5aNyp_ziZ6T5"},"source":["encoder_input = np.array(df.sbt)\n","decoder_input = np.array(df.bdd_input)\n","decoder_label = np.array(df.bdd_label)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FC32Fx4uZ6T5"},"source":["On repartit nos données en set de test et set d'entrainnement, on choisit de consacrer 90% des données pour l'entrainement et 10% pour le test :"]},{"cell_type":"code","metadata":{"id":"osaftL2HZ6T5"},"source":["total = num_folder\n","test_size = 0.1 #pourcentage des données de test \n","\n","train_encoder_input = encoder_input[:-int(total*test_size)]\n","train_decoder_input = decoder_input[:-int(total*test_size)]\n","train_decoder_label = decoder_label[:-int(total*test_size)]\n","\n","test_encoder_input = encoder_input[-int(total*test_size):]\n","test_decoder_input = decoder_input[-int(total*test_size):]\n","test_decoder_label = decoder_label[-int(total*test_size):]\n","'''\n","print(\"train dataset shape\")\n","print(train_encoder_input.shape)\n","print(train_decoder_input.shape)\n","print(train_decoder_label.shape)\n","\n","print(\"\\n\\ntest dataset shape\")\n","print(test_encoder_input.shape)\n","print(test_decoder_input.shape)\n","print(test_decoder_label.shape)\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FUCVGZK_Z6T6"},"source":["# Encodage des données"]},{"cell_type":"markdown","metadata":{"id":"xhhoyETyZ6T6"},"source":["Il faut maintenant encoder nos données, c'est a dire convertir ce texte à trous en une sequence de nombres, cette fonction prend en entrée notre texte à trous et des parametres pour l'encodage et renvoit une version encodée. "]},{"cell_type":"code","metadata":{"id":"SloMDJHDZ6T6"},"source":["def encode(text,word_to_ind, dim1, dim2,type):# type 0 encoder, 1 decoder_input, 2 decoder_label\n","    encoded_matrix = np.full((dim1,dim2),word_to_ind[' '])\n","    #print('Encoding Matrix shape : ',encoded_matrix.shape)\n","    for i,words in enumerate(text):\n","        j=0\n","        #print(vars)\n","        split_word = words.split(' ')\n","        if type == 1:\n","            encoded_matrix[i,j]=word_to_ind['sostoken '] #on ajoute le start of sentence token\n","            j=j+1\n","        \n","        for word in (split_word):\n","            encoded_matrix[i,j]=word_to_ind[word]\n","\n","            j=j+1\n","        if type == 2:\n","            encoded_matrix[i,j]=word_to_ind[' eostoken'] #on ajoute le end of sentence token\n","            j=j+1\n","\n","    return encoded_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u0zfU3sRZ6T6"},"source":["La fonction suivante permet d'encoder un seul texte à trous. "]},{"cell_type":"code","metadata":{"id":"ttsAkLCVZ6T7"},"source":["def encode_unique(text):# type 0 encoder, 1 decoder_input, 2 decoder_label\n","    encoded_matrix_unique = np.full(256,word_to_ind_sbt[' '])\n","\n","    split_word = text.split(' ')\n","\n","    for i,word in enumerate(split_word):\n","        encoded_matrix_unique[i]=word_to_ind_sbt[word]\n","\n","    return encoded_matrix_unique"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EHtpuKEdZ6T7"},"source":["encode_unique(txt_trou_unique)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4QuZ1WVZZ6T7"},"source":["Génère les textes à trous encodés avec les fonction précédentes"]},{"cell_type":"code","metadata":{"id":"nl_S7pZFZ6T7"},"source":["#print(\"train dataset shape\")\n","train_encoder_input_encoded = encode(train_encoder_input, word_to_ind_sbt,len(train_encoder_input),max_words_sbt,0)\n","train_decoder_input_encoded = encode(train_decoder_input, word_to_ind_bdd,len(train_decoder_input),max_words_bdd,1)\n","train_decoder_label_encoded = encode(train_decoder_label, word_to_ind_bdd,len(train_decoder_label),max_words_bdd,2)\n","#print(\"\\n\\ntest dataset shape\")\n","\n","test_encoder_input_encoded = encode(test_encoder_input, word_to_ind_sbt,len(test_encoder_input),max_words_sbt,0)\n","test_decoder_input_encoded = encode(test_decoder_input, word_to_ind_bdd,len(test_decoder_input),max_words_bdd,1)\n","test_decoder_label_encoded = encode(test_decoder_label, word_to_ind_bdd,len(test_decoder_label),max_words_bdd,2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w8JMhz9oZ6T7"},"source":["train_encoder_input_encoded.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a202FX0bZ6T8"},"source":["la fonction suivante permet de décoder, donc de passer d'un texte encodé a un texte lisible"]},{"cell_type":"code","metadata":{"id":"UnmqwdsWZ6T8"},"source":["def decode ( encoded_sequence, ind_to_char):\n","    decoded_sequence = []\n","    for var in encoded_sequence:\n","        decoded_sequence.append(ind_to_char[var])\n","    return ' '.join(decoded_sequence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rrenejEzZ6T8"},"source":["decode(train_decoder_label_encoded[0],ind_to_word_bdd)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V7mH--bTZ6T8"},"source":["# Création du modèle"]},{"cell_type":"code","metadata":{"id":"8UX0vNUJZ6T8"},"source":["from keras.layers import Input,Embedding,LSTM,Dense,Concatenate,Attention\n","from keras.models import Model\n","#from keras.utils import plot_model\n","from keras import backend as K\n","\n","#hyperparameters\n","embedding_size = 256\n","hidden_size = 256\n","\n","# trainer model (generator model will use the same encoder tho)\n","encoder_input = Input(shape=[max_words_sbt]) # size chain SBT \n","encoder_embedding = Embedding(max_words_sbt,embedding_size,mask_zero=True)\n","encoder_embedded = encoder_embedding(encoder_input)\n","\n","encoder_lstm1 = LSTM(hidden_size,return_sequences=True,return_state=True,dropout=0.2,recurrent_dropout=0)\n","encoder_output1,encoder_h1,encoder_c1 = encoder_lstm1(encoder_embedded)\n","\n","encoder_lstm2 = LSTM(hidden_size,return_sequences=True,return_state=True,dropout=0.2,recurrent_dropout=0)\n","encoder_output2,encoder_h2,encoder_c2 = encoder_lstm2(encoder_output1)\n","\n","encoder_lstm3 = LSTM(hidden_size,return_sequences=True,return_state=True,dropout=0.2,recurrent_dropout=0)\n","encoder_output3,encoder_h3,encoder_c3 = encoder_lstm3(encoder_output1)\n","\n","decoder_input = Input(shape=(None,))\n","decoder_embedding = Embedding(max_words_bdd,embedding_size,mask_zero=True) # len chain bdd\n","decoder_embedded = decoder_embedding(decoder_input)\n","\n","decoder_lstm = LSTM(hidden_size,return_sequences=True,return_state=True,dropout=0.2,recurrent_dropout=0)\n","decoder_output,_,_ = decoder_lstm(decoder_embedded,initial_state=[encoder_h3,encoder_c3])\n","\n","attn_layer = Attention()\n","attn_context = attn_layer([decoder_output,encoder_output3])\n","\n","decoder_output = Concatenate(axis=-1)([decoder_output,attn_context])\n","tanh_dense= Dense(hidden_size*2,activation=K.tanh)\n","decoder_output = tanh_dense(decoder_output)\n","\n","softmax_dense = Dense(number_of_words_bdd,activation='softmax')\n","decoder_output = softmax_dense(decoder_output)\n","\n","trainer_model = Model([encoder_input,decoder_input],decoder_output)\n","trainer_model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CKYdg7XpZ6T8"},"source":["La cellule suivante permet d'afficher un résumé du modele avec le nombre de neurone sur chaque couche."]},{"cell_type":"code","metadata":{"id":"uwRbO_3MZ6T9"},"source":["trainer_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wsD9bi2UZ6T9"},"source":["Entrainement du modèle :"]},{"cell_type":"code","metadata":{"id":"P0DjI2DJZ6T9"},"source":["print(\"\\nTraining of the model ... \\n\")\n","trainer_hist =trainer_model.fit([train_encoder_input_encoded,train_decoder_input_encoded],train_decoder_label_encoded,epochs=10,batch_size=128,validation_split=0.2)\n","print(\"\\nTraining completed ! \\n\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AjwZMQZvZ6T9"},"source":["Dans le cas ou on entraine le modèle, la cellule suivante affiche la précision du modele en focntion du nombre d'epochs, ce graphe est utile pour prévenir de l'overfitting:"]},{"cell_type":"code","metadata":{"id":"N4r9FRibZ6T9"},"source":["print(\"\\nSaving the model ... \\n\")\n","trainer_model.save_weights('template_attention_weigths.h5') # permet de sauvegarder les poids du modele\n","print(\"\\nModel saved !\\n\")\n","print(\"\\nPLoting the metrics / epochs ... \\n\")\n","losses = pd.DataFrame(trainer_model.history.history)\n","losses.plot()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dMTiEXCNZ6T-"},"source":["#trainer_model.load_weights('template_attention_weigths.h5') # permet de charger les poids"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E9lVMA6zZ6T-"},"source":["#generator model\n","gen_encoder = Model([encoder_input],[encoder_output3,encoder_h3,encoder_c3])\n","\n","gen_decoder_values_input = Input(shape=(max_words_sbt,hidden_size))\n","gen_decoder_h_input = Input(shape=[hidden_size])\n","gen_decoder_c_input = Input(shape=[hidden_size])\n","\n","gen_decoder_embedded = decoder_embedding(decoder_input)\n","gen_decoder_output,gen_decoder_h,gen_decoder_c = decoder_lstm(gen_decoder_embedded,initial_state=[gen_decoder_h_input,gen_decoder_c_input])\n","\n","attn_context = attn_layer([gen_decoder_output,gen_decoder_values_input])\n","gen_decoder_output = Concatenate(axis=-1)([gen_decoder_output,attn_context])\n","\n","gen_decoder_output = tanh_dense(gen_decoder_output)\n","gen_decoder_output = softmax_dense(gen_decoder_output)\n","\n","gen_decoder = Model([decoder_input]+[gen_decoder_values_input,gen_decoder_h_input,gen_decoder_c_input],[gen_decoder_output,gen_decoder_h,gen_decoder_c])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s0plOleopAFc"},"source":["# Test"]},{"cell_type":"code","metadata":{"id":"Odv8BLdYZ6T-"},"source":["def generate_from_encoder_input(encoder_input):\n","    encoder_input = encoder_input.reshape(1,-1)\n","    values,h,c = gen_encoder.predict(encoder_input)\n","    \n","    single_tok = np.zeros((1,1))\n","    single_tok[0,0] = word_to_ind_bdd['sostoken ']\n","    decoder_input = single_tok\n","    \n","    generated = []\n","    count = 0\n","    while(True):\n","        decoder_output,new_h,new_c = gen_decoder.predict([decoder_input]+[values,h,c])\n","        count +=1\n","        \n","        sampled_index = np.argmax(decoder_output[0,-1,:])\n","        sampled_word = ind_to_word_bdd[sampled_index]\n","        \n","        if sampled_word != ' eostoken' and sampled_index != 0:\n","            generated.append(sampled_word)\n","        if count >= max_words_bdd or sampled_word == ' eostoken':\n","            break\n","        \n","        h,c = new_h,new_c\n","        decoder_input[0,0] = sampled_index\n","    \n","    generated = ' '.join(generated)\n","    return generated"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ahhnw9bEZ6T_"},"source":["'''\n","for i in range(520,525):\n","    print(\"\\n<<sample encoder input SBT >>\")\n","    print(decode(train_encoder_input_encoded[i],ind_to_word_sbt))\n","    print(\"\\n\")\n","    print(\"<<generated BDD >>\")\n","    print(generate_from_encoder_input(train_encoder_input_encoded[i]))\n","    print(\"\\n\")\n","    print(\"<<Expected BDD>>\")\n","    print(decode(train_decoder_label_encoded[i],ind_to_word_bdd))\n","    print(\"========================================\\n\")\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vjEBizzlZ6T_"},"source":["input_encoded = test_encoder_input_encoded[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eZRao9rJZ6T_"},"source":["def return_generated_template(input_encoded):\n","    generated = generate_from_encoder_input(input_encoded)\n","    generated = generated.split(' ')\n","    return generated"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZjBtbanTZ6T_"},"source":["print(\"\\nEvaluating the model ... \\n\")\n","score = trainer_model.evaluate([train_encoder_input_encoded,train_decoder_input_encoded], train_decoder_label_encoded, verbose=0)\n","print(\"%s: %.2f%%\" % (trainer_model.metrics_names[1], score[1]*100))"],"execution_count":null,"outputs":[]}]}